{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import udhr  \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr to /home/dhruvdh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/udhr.zip.\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "eng = 0\n",
    "fre = 1\n",
    "ita = 2\n",
    "spa = 3\n",
    "\n",
    "nltk.download('udhr')\n",
    "\n",
    "lang = [eng, fre, ita, spa]\n",
    "languages = [\"English\", \"French\", \"Italian\", \"Spanish\"]\n",
    "\n",
    "english = udhr.raw('English-Latin1') \n",
    "french = udhr.raw('French_Francais-Latin1') \n",
    "italian = udhr.raw('Italian_Italiano-Latin1') \n",
    "spanish = udhr.raw('Spanish_Espanol-Latin1')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulate probabilty distribution from frequency distribution\n",
    "def pdf(fdist):\n",
    "    total = 0\n",
    "    for i in fdist:\n",
    "        total = total + fdist[i]\n",
    "    pdf = {}\n",
    "    for i in fdist:\n",
    "        pdf[i] = fdist[i]/total\n",
    "    return pdf\n",
    "\n",
    "#remove punctuations\n",
    "def remove_punct(text):\n",
    "    corpus = []\n",
    "    for character in text:\n",
    "        if character not in string.punctuation:\n",
    "            corpus.append(character)\n",
    "    return ''.join(corpus)\n",
    "    \n",
    "# remove punctuations and convert to lower case \n",
    "def preprocess(text):\n",
    "    text = text.strip()\n",
    "    text = remove_punct(text)\n",
    "    text = str.lower(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [preprocess(lang) for lang in [english, french, italian, spanish]]\n",
    "\n",
    "english = data[eng]\n",
    "french = data[fre]\n",
    "italian = data[ita]\n",
    "spanish = data[spa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test data for all languages\n",
    "train = [english[0:1000],\n",
    "         french[0:1000],\n",
    "         italian[0:1000],\n",
    "         spanish[0:1000]]\n",
    "\n",
    "test = [udhr.words('English-Latin1')[0:1000],\n",
    "        udhr.words('French_Francais-Latin1')[0:1000],\n",
    "        udhr.words('Italian_Italiano-Latin1')[0:1000], \n",
    "        udhr.words('Spanish_Espanol-Latin1')[0:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the test data\n",
    "test = [[preprocess(w) for w in test_set] for test_set in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram models computed \n",
    "Unigrams = [ngrams(train[language], 1) for language in lang]\n",
    "Bigrams = [ngrams(train[language], 2, ) for language in lang]\n",
    "Trigrams = [ngrams(train[language], 3 ) for language in lang] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating frequency distribution\n",
    "fdist_Uni = [nltk.FreqDist(unigram) for unigram in Unigrams]\n",
    "fdist_Bi = [nltk.FreqDist(bigram) for bigram in Bigrams]\n",
    "fdist_Tri = [nltk.FreqDist(trigram) for trigram in Trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculating probability distribution\n",
    "pdist_Uni = [pdf(fdist) for fdist in fdist_Uni]\n",
    "pdist_Bi = [pdf(fdist) for fdist in fdist_Bi]\n",
    "pdist_Tri = [pdf(fdist) for fdist in fdist_Tri]\n",
    "# print(\"*******\\nUnigram probability:\\n\",pdist_Uni)\n",
    "# print(\"*******\\nUnigram probability:\\n\",pdist_Bi)\n",
    "# print(\"*******\\nUnigram probability:\\n\",pdist_Tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulating the unigram model probabilty for a given language \n",
    "def unigram_probability(w, language):\n",
    "    p = 1\n",
    "    df = pdist_Uni[language]\n",
    "    for character in w:\n",
    "        key = (character,)\n",
    "        p_key = df[key] if key in df else 0\n",
    "        if p_key == 0:\n",
    "            return 0\n",
    "        p = p * p_key\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulating the bigram model probabilty for a given language \n",
    "def bigram_probabilty(w, language):\n",
    "    length = len(w)\n",
    "    if length < 2:\n",
    "             return unigram_probability(w, language)\n",
    "    p = 1\n",
    "    df = pdist_Bi[language]\n",
    "    for i in range(length+1):\n",
    "        if i == 0:\n",
    "            key = (' ',w[i])\n",
    "        elif i == length:\n",
    "            key == (w[i-1], ' ')\n",
    "        else:\n",
    "            key = (w[i-1],w[i])\n",
    "        p_key = df[key] if key in df else 0\n",
    "        if p_key == 0:\n",
    "            return 0\n",
    "        prev = ' ' if i == 0 else w[i-1]\n",
    "        p = p * p_key / unigram_probability(str(prev), language)\n",
    "    \n",
    "    return p;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calulating the trigram model probabilty for a given language \n",
    "def trigram_probability(w, language):\n",
    "    length = len(w)\n",
    "    if length < 3:\n",
    "        return bigram_probabilty(w, language)\n",
    "    p = 1\n",
    "    df = pdist_Tri[language]\n",
    "    for i in range(1, length+1):\n",
    "        if i == 1:\n",
    "            key = (' ', w[i-1], w[i])\n",
    "        elif i == length:\n",
    "            key = (w[i-2], w[i-1], ' ')\n",
    "        else:\n",
    "            key = (w[i-2], w[i-1], w[i])\n",
    "        \n",
    "        p_key = df[key] if key in df else 0\n",
    "        \n",
    "        if p_key == 0:\n",
    "            return 0\n",
    "        key2 = (' ', w[i-1]) if i == 1 else (w[i-2], w[i-1])\n",
    "        p_key2 = pdist_Bi[language][key2]\n",
    "        p = p * p_key / p_key2\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating accuracy\n",
    "def predict(test_set, l1, l2):\n",
    "    n = len(test_set)\n",
    "    filename = languages[l1] + \"_\" + languages[l2] +\"_\" + str(n) + \".txt\"\n",
    "    umatches = 0\n",
    "    bmatches = 0\n",
    "    tmatches = 0\n",
    "    with open(filename, 'w') as f:\n",
    "        for word in test_set:\n",
    "            p1 = unigram_probability(word, l1)\n",
    "            p2 = unigram_probability(word, l2)\n",
    "        \n",
    "            p3 = bigram_probabilty(word, l1)\n",
    "            p4 = bigram_probabilty(word, l2)\n",
    "        \n",
    "            p5 = trigram_probability(word, l1)\n",
    "            p6 = trigram_probability(word, l2)\n",
    "    \n",
    "            if p1 >= p2:\n",
    "                umatches = umatches + 1\n",
    "        \n",
    "            if p3 >= p4:\n",
    "                bmatches = bmatches + 1\n",
    "        \n",
    "            if p5 >= p6:\n",
    "                tmatches = tmatches + 1\n",
    "\n",
    "    uni_probability = umatches * 100 / n\n",
    "    bi_probability =  bmatches * 100 / n\n",
    "    tri_probability = tmatches * 100 / n\n",
    "    \n",
    "    print(\"\\n\\nAccuracy: \" + languages[l1] + \" vs \" + languages[l2] + \"\\n\")\n",
    "    print(\"Unigram: = {0:.5f}\".format(uni_probability))\n",
    "    print(\"Bigram: = {0:.5f}\".format(bi_probability))\n",
    "    print(\"Trigram: = {0:.5f}\".format(tri_probability))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******TEST SETS*********\n",
      "\n",
      "\n",
      "Accuracy: English vs French\n",
      "\n",
      "Unigram: = 78.20000\n",
      "Bigram: = 91.80000\n",
      "Trigram: = 99.00000\n",
      "\n",
      "\n",
      "Accuracy: Spanish vs Italian\n",
      "\n",
      "Unigram: = 67.00000\n",
      "Bigram: = 84.80000\n",
      "Trigram: = 96.30000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n*******TEST SETS*********\")\n",
    "#Question 1\n",
    "predict(test[eng], eng, fre);\n",
    "#Question 2\n",
    "predict(test[spa], spa, ita);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed from the accuracies above, it is clear that the language Spanish vs Italian is harder to distinguish. This is because the characters of both the language are significantly different from each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
